{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/g/g-u1ZJsrG7f-brainstorming-six-thinking-hats/c/24256d6d-8d12-4405-82e2-f59b3e759348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from reduce_keys import reduce_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neo4jConnection import Neo4jConnection\n",
    "\n",
    "graphdb = Neo4jConnection(uri=\"neo4j://localhost:7687\", user=\"neo4j\", password=\"password\", db=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_products(path):\n",
    "    products_df = pd.read_csv(path)\n",
    "    products_df[\"categories\"] = products_df[\"categories\"].apply(literal_eval)\n",
    "    products_df[\"details\"] = products_df[\"details\"].apply(literal_eval)\n",
    "    products_df[\"description\"] = products_df[\"description\"].apply(literal_eval)\n",
    "    products_df[\"features\"] = products_df[\"features\"].apply(literal_eval)\n",
    "    return products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = load_products(\"data/products_0.001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Beauty_and_Personal_Care\"\n",
    "dataset_name = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "ds_type = (\"raw_review_\", \"raw_meta_\")\n",
    "\n",
    "dataset = load_dataset(dataset_name, ds_type[1] + category, trust_remote_code=True, token=token, split=\"full\")\n",
    "df = dataset.to_pandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Drop columns that are not needed\n",
    "df.drop(columns=[\"videos\", \"bought_together\", \"subtitle\", \"author\"], inplace=True)\n",
    "\n",
    "# Step 1: Replace \"None\" with np.nan in specific object columns except for features, images, description, categories, details\n",
    "columns_to_check = [\"main_category\", \"title\", \"price\", \"store\", \"parent_asin\"]\n",
    "df[columns_to_check] = df[columns_to_check].replace(\"None\", np.nan)\n",
    "\n",
    "# Step 2: Drop rows where any of the specified columns is null\n",
    "# capture the digits in price and convert to float\n",
    "df[\"price\"] = df[\"price\"].str.extract(r\"(\\d+\\.\\d+|\\d+)\").astype(float)\n",
    "df.dropna(\n",
    "    subset=[\"main_category\", \"title\", \"price\", \"images\", \"store\", \"parent_asin\"],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Remove \"Beauty & Personal Care\" from each array in 'categories'\n",
    "df['categories'] = df['categories'].apply(lambda x: [cat for cat in x if cat != \"Beauty & Personal Care\"])\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Drop rows based on any of the length conditions\n",
    "- if len() of the values in images, categories, or details is 0\n",
    "- if both description and features have length 0\n",
    "\"\"\"\n",
    "df = df[\n",
    "    (df[\"images\"].apply(len) > 0)\n",
    "    & (df[\"details\"].apply(len) > 0)\n",
    "    & (df[\"categories\"].apply(len) > 0)\n",
    "    & ~((df[\"description\"].apply(len) == 0) & (df[\"features\"].apply(len) == 0))\n",
    "]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explicit KG from Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Main Category nodes (`main_category`). Linked with `Subcategory` or `Product` nodes.\n",
    "2. Subcategory nodes (`categories`). `BELONGS_TO` `Category` and linked with `Product` nodes.\n",
    "3. Product nodes (`title`, `average_rating`, `rating_number`, `features`, `description`, `price`, `images`, `store`, `id`). `BELONGS_TO` `Subcategory` or `Category`\n",
    "4. Price Range ($5 ranges based on unique values in `price`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_category'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant = df[\n",
    "    df[\"main_category\"].isin(\n",
    "        [\n",
    "            \"All Beauty\",\n",
    "            \"Health & Personal Care\",\n",
    "            \"Amazon Home\",\n",
    "            \"AMAZON FASHION\",\n",
    "            \"Premium Beauty\",\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "final_sampled_df = df_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the categories column to get one row per category\n",
    "df_exploded = df_relevant.explode(\"categories\")\n",
    "\n",
    "# Stratified sampling by category values\n",
    "# First, get a unique set of categories per product and stratify by these unique values\n",
    "sample_frac = 0.001\n",
    "sampled_df = (\n",
    "    df_exploded.groupby(\"categories\")\n",
    "    .sample(frac=sample_frac)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# After stratified sampling, remove duplicates to get the original product_asin sample\n",
    "sampled_products = sampled_df[[\"parent_asin\"]].drop_duplicates()\n",
    "\n",
    "# Join back with the original dataframe to get the full sampled products with their category arrays\n",
    "final_sampled_df = df[df[\"parent_asin\"].isin(sampled_products[\"parent_asin\"])].reset_index(drop=True)\n",
    "\n",
    "print(\"Sample Size:\", final_sampled_df.shape[0] / df_relevant.shape[0] * 100)\n",
    "final_sampled_df.info()\n",
    "final_sampled_df['main_category'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all Category nodes\n",
    "graphdb.query(\"MATCH (c:Category) DETACH DELETE c\")\n",
    "\n",
    "# Get unique values from the main_category column\n",
    "unique_categories = final_sampled_df['main_category'].unique()\n",
    "\n",
    "# Loop through each unique category and create a Category node\n",
    "queries = []\n",
    "for category in unique_categories:\n",
    "    # Cypher query to create a Category node\n",
    "    query = f\"\"\"\n",
    "    CREATE (c:Category {{name: '{category}'}})\n",
    "    \"\"\"\n",
    "    # add to the batch of queries\n",
    "    queries.append(query)\n",
    "# Execute the queries\n",
    "graphdb.bulk_query(queries)\n",
    "\n",
    "unique_category_count = final_sampled_df[\"main_category\"].nunique()\n",
    "query = \"MATCH (c:Category) RETURN COUNT(c) as category_count\"\n",
    "result, _ = graphdb.query(query)\n",
    "# Extract the count from the result\n",
    "category_node_count = result[0][\"category_count\"]\n",
    "\n",
    "is_equal = unique_category_count == category_node_count\n",
    "print(f\"Number of Category nodes ({category_node_count}) {'==' if is_equal else '!='} unique categories in the DataFrame ({unique_category_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing Subcategory nodes\n",
    "graphdb.query(\"MATCH (sc:Subcategory) DETACH DELETE sc\")\n",
    "\n",
    "# Explode the categories column into individual rows\n",
    "df_exploded = final_sampled_df.explode('categories')\n",
    "\n",
    "# Get the unique values from the categories column\n",
    "unique_subcategories = df_exploded['categories'].unique()\n",
    "\n",
    "# Loop through each unique subcategory and create a Subcategory node\n",
    "queries = []\n",
    "for category in unique_subcategories:\n",
    "    # escape single quotes in the category name\n",
    "    category = category.replace(\"'\", \"\\\\'\")\n",
    "    # Cypher query to create a Category node\n",
    "    query = f\"\"\"\n",
    "    CREATE (sc:Subcategory {{name: '{category}'}})\n",
    "    \"\"\"\n",
    "    # Add to the batch of queries\n",
    "    queries.append(query)\n",
    "# Execute the queries\n",
    "graphdb.bulk_query(queries)\n",
    "\n",
    "unique_subcategory_count = df_exploded[\"categories\"].nunique()\n",
    "query = \"MATCH (sc:Subcategory) RETURN COUNT(sc) as subcategory_count\"\n",
    "result, _ = graphdb.query(query)\n",
    "# Extract the count from the result\n",
    "subcategory_node_count = result[0][\"subcategory_count\"]\n",
    "# Check if the number of Category nodes matches the unique categories in the DataFrame\n",
    "is_equal = unique_subcategory_count == subcategory_node_count\n",
    "print(f\"Number of Subcategory nodes ({subcategory_node_count}) {'==' if is_equal else '!='} unique categories in the DataFrame ({unique_subcategory_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate (main_category, categories) pairs\n",
    "df_cat_subcat = df_exploded[['main_category', 'categories']].drop_duplicates()\n",
    "\n",
    "# Create the queries\n",
    "queries = []\n",
    "for index, row in df_cat_subcat.iterrows():\n",
    "    # Escape single quotes in both main_category and categories\n",
    "    main_category = row['main_category'].replace(\"'\", \"\\\\'\")\n",
    "    subcategory = row['categories'].replace(\"'\", \"\\\\'\")\n",
    "\n",
    "    # Cypher query to create Subcategory and Category nodes and the BELONGS_TO relationship\n",
    "    query = f\"\"\"\n",
    "    MERGE (c:Category {{name: '{main_category}'}})\n",
    "    MERGE (sc:Subcategory {{name: '{subcategory}'}})\n",
    "    MERGE (sc)-[:BELONGS_TO]->(c)\n",
    "    \"\"\"\n",
    "    # Add to the batch of queries\n",
    "    queries.append(query)\n",
    "# Execute the queries\n",
    "graphdb.bulk_query(queries);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_images(images):\n",
    "    images_simple = defaultdict(list)\n",
    "    for key in images:\n",
    "        for link in images[key]:\n",
    "            if link and link.startswith(\"https://\"):\n",
    "                images_simple[key].append(link)\n",
    "    return images_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = []\n",
    "relationships = []\n",
    "\n",
    "for index, row in final_sampled_df.iterrows():\n",
    "    product_id = row[\"parent_asin\"]\n",
    "    title = row[\"title\"].replace(\"'\", \"\\\\'\")\n",
    "    average_rating = row[\"average_rating\"]\n",
    "    rating_number = row[\"rating_number\"]\n",
    "    price = row[\"price\"]\n",
    "    images = json.dumps(simplify_images(row[\"images\"]))\n",
    "    store = row[\"store\"].replace(\"'\", \"\\\\'\")\n",
    "    subcategories = row[\"categories\"]\n",
    "    subcategories = [sc.replace(\"'\", \"\\\\'\") for sc in subcategories]\n",
    "\n",
    "    # Collect product data\n",
    "    product_data.append({\n",
    "        'product_id': product_id,\n",
    "        'title': title,\n",
    "        'average_rating': average_rating,\n",
    "        'rating_number': rating_number,\n",
    "        'price': price,\n",
    "        'images': images,\n",
    "        'store': store\n",
    "    })\n",
    "\n",
    "    # Collect relationships\n",
    "    for subcategory in subcategories:\n",
    "        relationships.append({\n",
    "            'product_id': product_id,\n",
    "            'subcategory': subcategory\n",
    "        })\n",
    "\n",
    "# Cypher query using UNWIND for bulk creation\n",
    "product_query = \"\"\"\n",
    "UNWIND $products AS product\n",
    "CREATE (p:Product {product_id: product.product_id})\n",
    "SET p.title = product.title,\n",
    "    p.average_rating = product.average_rating,\n",
    "    p.rating_number = product.rating_number,\n",
    "    p.price = product.price,\n",
    "    p.images = product.images,\n",
    "    p.store = product.store\n",
    "\"\"\".strip()\n",
    "\n",
    "relationship_query = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (p:Product {product_id: rel.product_id})\n",
    "MATCH (sc:Subcategory {name: rel.subcategory})\n",
    "MERGE (p)-[:BELONGS_TO]->(sc)\n",
    "\"\"\".strip()\n",
    "\n",
    "# Run the queries\n",
    "batch_size = 1000\n",
    "\n",
    "# Delete existing Review nodes\n",
    "delete_query = \"\"\"\n",
    "MATCH (p:Product)\n",
    "CALL (p) { \n",
    "DETACH DELETE p\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "graphdb.query(delete_query)\n",
    "# Create the Product nodes\n",
    "for i in trange(0, len(product_data), batch_size):\n",
    "    batch = product_data[i:i + batch_size]\n",
    "    graphdb.query(product_query, products=batch)\n",
    "\n",
    "graphdb.query(\"DROP INDEX product_range_productid IF EXISTS\")\n",
    "graphdb.query(\"DROP INDEX subcategory_range_name IF EXISTS\")\n",
    "graphdb.query(\"CREATE INDEX product_range_productid FOR (p:Product) ON (p.product_id)\")\n",
    "graphdb.query(\"CREATE INDEX subcategory_range_name FOR (sc:Subcategory) ON (sc.name)\")\n",
    "\n",
    "for i in trange(0, len(relationships), batch_size):\n",
    "    batch = relationships[i:i + batch_size]\n",
    "    graphdb.query(relationship_query, relationships=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_range(price, change_point=550, percentage=10):\n",
    "    if price <= change_point:\n",
    "        # Use percentage-based interval\n",
    "        lower_bound = price * (1 - percentage / 100)\n",
    "        upper_bound = price * (1 + percentage / 100)\n",
    "    else:\n",
    "        # Use logarithmic-based interval\n",
    "        log_base = 1.1  # You can adjust this base for more or less aggressive intervals\n",
    "        lower_bound = price / log_base\n",
    "        upper_bound = price * log_base\n",
    "    \n",
    "    return int(lower_bound), int(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_ranges = set()\n",
    "price_relationships = []\n",
    "\n",
    "# Step 1: Iterate over each row in the DataFrame\n",
    "for index, row in final_sampled_df.iterrows():\n",
    "    product_id = row[\"parent_asin\"]\n",
    "    price = row[\"price\"]\n",
    "\n",
    "    # Step 2: Create price ranges\n",
    "    lower_bound, upper_bound = get_price_range(price, percentage=15)\n",
    "\n",
    "    # Add the price range to the set\n",
    "    price_ranges.add((lower_bound, upper_bound))\n",
    "\n",
    "    # Step 3: Collect relationships between product and price range\n",
    "    price_relationships.append(\n",
    "        {\n",
    "            \"product_id\": product_id,\n",
    "            \"lower_bound\": lower_bound,\n",
    "            \"upper_bound\": upper_bound,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Step 4: Insert unique PriceRange nodes\n",
    "price_range_query = \"\"\"\n",
    "UNWIND $price_ranges AS pr\n",
    "CREATE (r:PriceRange {lower_limit: pr.lower_bound, upper_limit: pr.upper_bound})\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the price_ranges data for Cypher\n",
    "price_range_data = [{\"lower_bound\": pr[0], \"upper_bound\": pr[1]} for pr in price_ranges]\n",
    "\n",
    "# Insert PriceRange nodes\n",
    "graphdb.query(\"MATCH (r:PriceRange) DETACH DELETE r\")\n",
    "graphdb.query(price_range_query.strip(), price_ranges=price_range_data)\n",
    "\n",
    "# Step 5: Create Cypher query to link Products to PriceRange nodes\n",
    "price_relationship_query = \"\"\"\n",
    "UNWIND $price_relationships AS rel\n",
    "MATCH (p:Product {product_id: rel.product_id})\n",
    "MATCH (r:PriceRange {lower_limit: rel.lower_bound, upper_limit: rel.upper_bound})\n",
    "MERGE (p)-[:AROUND_PRICE]->(r)\n",
    "\"\"\"\n",
    "\n",
    "# Insert AROUND_PRICE relationships\n",
    "for i in trange(0, len(price_relationships), batch_size):\n",
    "    batch = price_relationships[i : i + batch_size]\n",
    "    graphdb.query(\n",
    "        price_relationship_query.strip(), price_relationships=price_relationships\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sampled_df.to_csv(f\"data/products_{sample_frac}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Explicit KG from Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Reviewer`: user_id\n",
    "- `Review`: rating, title, text, helpful_vote\n",
    "- `REVIEWS`: based on user_id\n",
    "- `WROTE`: based on parent_asin\n",
    "\n",
    "The purpose is just stylistic. That is, whenever a user clicks a product, they see\n",
    "- The most helpful reviews (or most recent if we're out of helpful ones)\n",
    "- What people are saying about this (LLM-extracted keywords & summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json(f\"data/reviews_0.001.json\")\n",
    "reviews_df.reset_index(inplace=True)\n",
    "reviews_df.info()\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort the DataFrame by 'parent_asin', 'helpful_vote' (descending), and 'timestamp' (descending)\n",
    "# reviews_df_sorted = reviews_df.sort_values(by=['parent_asin', 'helpful_vote', 'timestamp'], ascending=[True, False, False])\n",
    "\n",
    "# # Group by 'parent_asin' and take the top 5 reviews from each group\n",
    "# reviews_df_top5 = reviews_df_sorted.groupby('parent_asin').head(5).reset_index(drop=True)\n",
    "# reviews_df_top5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique user_ids\n",
    "unique_users = reviews_df[\"user_id\"].unique()\n",
    "unique_users = [{\"user_id\": user} for user in unique_users]\n",
    "print(\"Number of unique users:\", len(unique_users))\n",
    "# Create Reviewer nodes\n",
    "graphdb.query(\"MATCH (r:Reviewer) DETACH DELETE r\")\n",
    "cypher = \"\"\"\n",
    "UNWIND $users AS reviewer\n",
    "CREATE (r:Reviewer {user_id: reviewer.user_id})\n",
    "\"\"\"\n",
    "batch_size = 1000\n",
    "for i in trange(0, len(unique_users), batch_size):\n",
    "    batch = unique_users[i:i + batch_size]\n",
    "    graphdb.query(cypher, users=batch)\n",
    "\n",
    "# Count number of reviewers in graph\n",
    "query = \"MATCH (r:Reviewer) RETURN COUNT(r) as reviewer_count\"\n",
    "result, _ = graphdb.query(query)\n",
    "reviewer_count = result[0][\"reviewer_count\"]\n",
    "print(\"Number of Reviewer nodes:\", reviewer_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = []\n",
    "review_product_rels = []\n",
    "review_reviewer_rels = []\n",
    "\n",
    "for index, row in tqdm(reviews_df.iterrows(), total=reviews_df.shape[0]):\n",
    "    review_id = row[\"index\"]\n",
    "    rating = row[\"rating\"]\n",
    "    try:\n",
    "        title = row[\"title\"].replace(\"'\", \"\\\\'\")\n",
    "        text = row[\"text\"].replace(\"'\", \"\\\\'\")\n",
    "        timestamp = row[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        continue\n",
    "    product_id = row[\"parent_asin\"]\n",
    "    reviewer_id = row[\"user_id\"]\n",
    "    helpful_vote = row[\"helpful_vote\"]\n",
    "\n",
    "    # Collect review data\n",
    "    reviews_data.append(\n",
    "        {\n",
    "            \"review_id\": review_id,\n",
    "            \"rating\": rating,\n",
    "            \"title\": title,\n",
    "            \"text\": text,\n",
    "            \"helpful_vote\": helpful_vote,\n",
    "            \"timestamp\": timestamp,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Collect relationships\n",
    "    review_product_rels.append({\"review_id\": review_id, \"product_id\": product_id})\n",
    "    review_reviewer_rels.append({\"review_id\": review_id, \"reviewer_id\": reviewer_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing Review nodes\n",
    "delete_query = \"\"\"\n",
    "MATCH (r:Review)\n",
    "CALL (r) { \n",
    "DETACH DELETE r\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "graphdb.query(delete_query)\n",
    "\n",
    "# Create Review nodes\n",
    "batch_size = 1000\n",
    "review_query = \"\"\"\n",
    "UNWIND $reviews AS review\n",
    "CREATE (r:Review {review_id: review.review_id, rating: review.rating, title: review.title, text: review.text, helpful_vote: review.helpful_vote, timestamp: review.timestamp})\n",
    "\"\"\"\n",
    "for i in trange(0, len(reviews_data), batch_size):\n",
    "    batch = reviews_data[i : i + batch_size]\n",
    "    graphdb.query(review_query, reviews=batch)\n",
    "\n",
    "# Drop existing indexes\n",
    "graphdb.query(\"DROP INDEX review_range_reviewid IF EXISTS\")\n",
    "graphdb.query(\"DROP INDEX reviewer_range_userid IF EXISTS\")\n",
    "# Create index on Review nodes\n",
    "graphdb.query(\"CREATE INDEX review_range_reviewid FOR (r:Review) ON (r.review_id)\")\n",
    "graphdb.query(\"CREATE INDEX reviewer_range_userid FOR (r:Reviewer) ON (r.user_id)\")\n",
    "\n",
    "# Create edge relationships between Review and Product nodes\n",
    "review_product_query = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (r:Review) WHERE r.review_id = rel.review_id\n",
    "MATCH (p:Product) WHERE p.product_id = rel.product_id\n",
    "CREATE (r)-[:REVIEWS]->(p)\n",
    "\"\"\"\n",
    "for i in trange(0, len(review_product_rels), batch_size):\n",
    "    batch = review_product_rels[i : i + batch_size]\n",
    "    graphdb.query(review_product_query, relationships=batch)\n",
    "# Create edge relationships between Review and Reviewer nodes\n",
    "review_reviewer_query = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (r:Review) WHERE r.review_id = rel.review_id\n",
    "MATCH (rv:Reviewer) WHERE rv.user_id = rel.reviewer_id\n",
    "CREATE (rv)-[:WROTE]->(r)\n",
    "\"\"\"\n",
    "for i in trange(0, len(review_reviewer_rels), batch_size):\n",
    "    batch = review_reviewer_rels[i : i + batch_size]\n",
    "    graphdb.query(review_reviewer_query, relationships=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries from review to reviewer will not be needed\n",
    "graphdb.query(\"DROP INDEX reviewer_range_userid IF EXISTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Information from Products via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract (via LLM) **SEO-optimized keywords** from title, features, and description. Create `Keyword` nodes. Create `HAS_KEYWORD` rel b/w `Keyword` and `Products`\n",
    "- Create (via LLM) **summary** of product from all of its attributes using a template. Then, create **embedding** of this summary for vector search. Use it to find products that are **substitutes** or **complements** of a product. A product is a substitute if it belongs to the same (or similar) categories, otherwise it is a complement. Complement can also be determined (better) via `bought_together`, however this is usually NULL. Another (hacky) technique for complements is to use an LLM to suggest titles for complements (\"complements\", \"frequently used with\", or \"often mentioned together\"), adding these edges to the KG, and when searching for complements, use full text search to find products that match these titles.  \n",
    "- Incorporate use case scenarios into the KG to provide more intuitive and relatable explanations for users. For example, connecting the query “insomnia” to common use cases like “taking an Epsom salt bath before bed” or “diffusing lavender oil at night” creates a more meaningful connection between the product and the user’s problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEO-optimized keyword extraction\n",
    "\n",
    "Extract (via LLM) **SEO-optimized keywords** from title, features, and description. Create `Keyword` nodes. Create `HAS_KEYWORD` rel b/w `Keyword` and `Products`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from typing import List\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv(\"data/products_0.001.csv\")\n",
    "products_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"product_title\": \"Hydrating Facial Serum\",\n",
    "        \"product_description\": \"A lightweight serum that deeply hydrates the skin, reducing the appearance of fine lines and wrinkles.\",\n",
    "        \"product_features\": \"Hydrating, anti-aging, lightweight, non-greasy, fast-absorbing\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"hydrating facial serum\",\n",
    "            \"anti-aging serum\",\n",
    "            \"lightweight serum\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"deeply hydrating skincare\",\n",
    "            \"fine line reducer\",\n",
    "            \"non-greasy facial treatment\",\n",
    "            \"fast-absorbing serum\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Organic Shea Butter Body Lotion\",\n",
    "        \"product_description\": \"Rich and nourishing body lotion made with organic shea butter to moisturize and soften the skin.\",\n",
    "        \"product_features\": \"Organic, shea butter, moisturizing, rich texture, skin-softening\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"organic shea butter lotion\",\n",
    "            \"moisturizing body lotion\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"rich body moisturizer\",\n",
    "            \"natural skin softener\",\n",
    "            \"nourishing body cream\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Vitamin C Brightening Moisturizer\",\n",
    "        \"product_description\": \"A daily moisturizer infused with Vitamin C to brighten the complexion and protect against environmental damage.\",\n",
    "        \"product_features\": \"Vitamin C, brightening, daily moisturizer, antioxidant, SPF protection\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"Vitamin C moisturizer\",\n",
    "            \"brightening facial cream\",\n",
    "            \"daily antioxidant moisturizer\",\n",
    "            \"SPF protected moisturizer\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"complexion brightener\",\n",
    "            \"environmental protection moisturizer\",\n",
    "            \"daily skin defender\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Matte Finish Lipstick\",\n",
    "        \"product_description\": \"Long-lasting lipstick with a matte finish, available in a variety of vibrant colors.\",\n",
    "        \"product_features\": \"Matte finish, long-lasting, vibrant colors, non-drying, cruelty-free\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"matte lipstick\",\n",
    "            \"long-lasting lip color\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"vibrant lipstick shades\",\n",
    "            \"non-drying makeup\",\n",
    "            \"cruelty-free lip products\",\n",
    "            \"bold color lipstick\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Exfoliating Facial Scrub\",\n",
    "        \"product_description\": \"A gentle exfoliating scrub that removes dead skin cells and unclogs pores, leaving the skin smooth and refreshed.\",\n",
    "        \"product_features\": \"Exfoliating, gentle, unclogs pores, smooth skin, refreshing\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"exfoliating facial scrub\",\n",
    "            \"gentle skin exfoliant\",\n",
    "            \"smooth skin facial scrub\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"pore unclogging scrub\",\n",
    "            \"refreshing exfoliator\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Anti-Pollution Facial Mist\",\n",
    "        \"product_description\": \"A refreshing facial mist that protects skin from urban pollution and hydrates throughout the day.\",\n",
    "        \"product_features\": \"Anti-pollution, hydrating, refreshing, lightweight, convenient spray bottle\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"anti-pollution facial mist\",\n",
    "            \"hydrating facial mist\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"urban pollution protection\",\n",
    "            \"daily hydration spray\",\n",
    "            \"lightweight skin mist\",\n",
    "            \"refreshing skin mist\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Organic Aloe Vera Gel\",\n",
    "        \"product_description\": \"Pure organic aloe vera gel to soothe and moisturize irritated skin.\",\n",
    "        \"product_features\": \"Organic, aloe vera, soothing, moisturizing, natural\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"organic aloe vera gel\",\n",
    "            \"soothing skin gel\",\n",
    "            \"moisturizing aloe gel\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"natural skin soother\",\n",
    "            \"irritated skin moisturizer\",\n",
    "            \"pure aloe treatment\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Sunscreen SPF 50\",\n",
    "        \"product_description\": \"Broad-spectrum sunscreen with SPF 50 protection to shield skin from harmful UV rays.\",\n",
    "        \"product_features\": \"Broad-spectrum, SPF 50, water-resistant, lightweight, non-greasy\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"sunscreen SPF 50\",\n",
    "            \"broad-spectrum sunscreen\",\n",
    "            \"water-resistant sunscreen\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"UV protection cream\",\n",
    "            \"lightweight sunblock\",\n",
    "            \"non-greasy sun protection\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Retinol Night Cream\",\n",
    "        \"product_description\": \"A potent night cream enriched with retinol to promote skin renewal and reduce wrinkles.\",\n",
    "        \"product_features\": \"Retinol, night cream, skin renewal, wrinkle reduction, nourishing\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"retinol night cream\",\n",
    "            \"skin renewal cream\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"overnight wrinkle reducer\",\n",
    "            \"nourishing night moisturizer\",\n",
    "            \"retinol enriched cream\",\n",
    "            \"anti-aging night treatment\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"product_title\": \"Clarifying Acne Spot Treatment\",\n",
    "        \"product_description\": \"Targeted spot treatment to reduce acne and prevent future breakouts.\",\n",
    "        \"product_features\": \"Clarifying, acne treatment, spot treatment, quick-drying, non-irritating\",\n",
    "        \"extractive_keywords\": [\n",
    "            \"clarifying acne treatment\",\n",
    "            \"spot treatment for acne\",\n",
    "            \"acne spot treatment\"\n",
    "        ],\n",
    "        \"abstractive_keywords\": [\n",
    "            \"breakout reducer\",\n",
    "            \"quick-drying acne cream\",\n",
    "            \"non-irritating blemish treatment\"\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    examples[i][\"product_features\"] = example[\"product_features\"].split(\", \")\n",
    "\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    template=\"\"\"Product:\n",
    "- Title: {product_title}\n",
    "- Description: {product_description}\n",
    "- Features: {product_features}\n",
    "Common Search Terms:\n",
    "{{{{\n",
    "\"extractive_keywords\": {extractive_keywords},\n",
    "\"abstractive_keywords\": {abstractive_keywords}\n",
    "}}}}\n",
    "\"\"\".strip(),\n",
    "    input_variables=[\n",
    "        \"product_title\",\n",
    "        \"product_description\",\n",
    "        \"product_features\",\n",
    "        \"extractive_keywords\",\n",
    "        \"abstractive_keywords\",\n",
    "    ],\n",
    "    input_types={\"product_title\":  str, \"product_description\": str, \"product_features\": str, \"extractive_keywords\": List[str], \"abstractive_keywords\": List[str]},\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"\"\"You are an expert in SEO-optimized keyword extraction. Your task is to extract common search terms that will make the product more discoverable in search engines.\n",
    "Instructions:\n",
    "\n",
    "Extract two types of keywords\n",
    "\n",
    "1. extractive_keywords: List[str] = 'List of extractive SEO-optimized keywords (3-5). MUST be present in the product description.'\n",
    "2. abstractive_keywords: List[str] = '''List of abstractive SEO-optimized keywords/phrases (3-5) AND\n",
    "    MUST NOT be present in the product description BUT MUST BE related to the product AND\n",
    "    MUST BE diverse AND NOT overlapping with the extractive keywords OR each other.'''\n",
    "\n",
    "Guidelines:\n",
    "1. **Relevance**: Extract keywords that are commonly used by potential customers searching for this product. Ensure the keywords are highly relevant to the product's features, use cases, and audience.\n",
    "2. **Avoid Keyword Overlap**: If there are multiple synonyms, choose only the most relevant keyword. For example, if the product description contains both \"cheap laptop\" and \"affordable laptop,\" choose one to avoid overlap and dilution.  \n",
    "3. **Edge Weighting (Priority)**: Consider how specific and unique the keyword is to the product. More specific and descriptive terms (e.g., \"lightweight laptop\" for a travel laptop) should be prioritized over generic terms (e.g., \"device\").\n",
    "\n",
    "Some examples are given below.\n",
    "\"\"\",\n",
    "    suffix=\"\"\"\n",
    "Your task: Extract the common search terms for the following product only.\n",
    "Product:\n",
    "- Title: {product_title}\n",
    "- Description: {product_description}\n",
    "- Features: {product_features}\n",
    "Common Search Terms:\n",
    "\"\"\",\n",
    "    input_variables=[\"product_title\", \"product_description\", \"product_features\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral-nemo\",\n",
    "    temperature=0.3, \n",
    ")\n",
    "chain = few_shot_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_llm = llm.with_structured_output(KeywordExtraction)\n",
    "# structured_chain = few_shot_prompt | keyword_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Example Usage\n",
    "input_data = {\n",
    "    \"product_title\": \"Hydrating Overnight Mask\",\n",
    "    \"product_description\": \"An intensive overnight mask that restores moisture and revitalizes tired skin, leaving it soft and radiant by morning.\",\n",
    "    \"product_features\": \"Hydrating, overnight treatment, revitalizing, moisture restoration, radiant skin\"\n",
    "}\n",
    "\n",
    "# Invoke the chain with the input data\n",
    "result = chain.invoke(input_data)\n",
    "print(result.content)\n",
    "# structured_result = structured_chain.invoke(input_data)\n",
    "\n",
    "# print(structured_result.extractive_keywords)\n",
    "# print(structured_result.abstractive_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_filename = \"keywords.pkl\"\n",
    "\n",
    "if os.path.exists(keywords_filename):\n",
    "    with open(keywords_filename, \"rb\") as f:\n",
    "        keywords = pickle.load(f)\n",
    "else:\n",
    "    keywords = []\n",
    "\n",
    "errors = []\n",
    "\n",
    "for index, row in tqdm(products_df.iterrows(), total=products_df.shape[0]):\n",
    "    if index < len(keywords):\n",
    "        continue\n",
    "\n",
    "    product_title = row[\"title\"]\n",
    "    product_description = row[\"description\"]\n",
    "    product_features = row[\"features\"]\n",
    "\n",
    "    if not product_description:\n",
    "        product_description = \"No description available.\"        \n",
    "    \n",
    "    inputs = {\n",
    "        \"product_title\": product_title,\n",
    "        \"product_description\": product_description[:5000],\n",
    "        \"product_features\": product_features\n",
    "    }\n",
    "    try:\n",
    "        chain_output = chain.invoke(inputs)\n",
    "        # regex pattern to find dict\n",
    "        dict_pattern = re.compile(r\"\\{[^\\}]+\\}\")\n",
    "        # call the LLM to format the output in the correct format\n",
    "        prompt = f\"\"\"\n",
    "Please format the below AI response into the following format:\n",
    "{{\n",
    "    \"extractive_keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n",
    "    \"abstractive_keywords\": [\"keyword4\", \"keyword5\", \"keyword6\"]\n",
    "}}\n",
    "If there is more than one dictionary, just keep the first one.\n",
    "\n",
    "{chain_output.content}\n",
    "\"\"\".strip()\n",
    "        llm_output = llm.invoke(prompt)\n",
    "        parsed_output = dict_pattern.search(llm_output.content)\n",
    "        if parsed_output is None:\n",
    "            raise ValueError(f\"Failed to parse the output for product {index}\")\n",
    "        # remove 's from the output to prevent issues with literal_eval\n",
    "        content = parsed_output.group().replace(\"'s \", \"s \")\n",
    "        parsed_output = literal_eval(content)\n",
    "        extractive_keywords = parsed_output['extractive_keywords']\n",
    "        abstractive_keywords = parsed_output['abstractive_keywords']\n",
    "        keywords.append({\"extractive\": extractive_keywords, \"abstractive\": abstractive_keywords})\n",
    "        with open(keywords_filename, \"wb\") as f:\n",
    "            pickle.dump(keywords, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {index}: {e}\")\n",
    "        errors.append(index)\n",
    "        keywords.append(None)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"errors_keywords.txt\") as f:\n",
    "    errors = f.read().splitlines()\n",
    "    errors = [int(e) for e in errors]\n",
    "\n",
    "for index in tqdm(errors):\n",
    "\n",
    "    row = products_df.loc[index]\n",
    "    product_title = row[\"title\"]\n",
    "    product_description = row[\"description\"]\n",
    "    product_features = row[\"features\"]\n",
    "\n",
    "    if not product_description:\n",
    "        product_description = \"No description available.\"\n",
    "\n",
    "    inputs = {\n",
    "        \"product_title\": product_title,\n",
    "        \"product_description\": product_description[:5000],\n",
    "        \"product_features\": product_features,\n",
    "    }\n",
    "    chain_output = chain.invoke(inputs)\n",
    "    # regex pattern to find dict\n",
    "    dict_pattern = re.compile(r\"\\{[^\\}]+\\}\")\n",
    "    # call the LLM to format the output in the correct format\n",
    "    prompt = f\"\"\"\n",
    "Please format the below AI response into the following format:\n",
    "{{\n",
    "\"extractive_keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n",
    "\"abstractive_keywords\": [\"keyword4\", \"keyword5\", \"keyword6\"]\n",
    "}}\n",
    "\n",
    "If there is more than one dictionary, just keep the first one.\n",
    "\n",
    "{chain_output.content}\n",
    "\"\"\".strip()\n",
    "    llm_output = llm.invoke(prompt)\n",
    "    parsed_output = dict_pattern.search(llm_output.content)\n",
    "    if parsed_output is None:\n",
    "        raise ValueError(f\"Failed to parse the output for product {index}\")\n",
    "    # remove 's from the output to prevent issues with literal_eval\n",
    "    content = parsed_output.group().replace(\"'s \", \"s \")\n",
    "    parsed_output = literal_eval(content)\n",
    "    extractive_keywords = parsed_output[\"extractive_keywords\"]\n",
    "    abstractive_keywords = parsed_output[\"abstractive_keywords\"]\n",
    "    keywords[index] = {\n",
    "        \"extractive\": extractive_keywords,\n",
    "        \"abstractive\": abstractive_keywords,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(keywords_filename, \"wb\") as f:\n",
    "    pickle.dump(keywords, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(keywords_filename, \"rb\") as f:\n",
    "    keywords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_entries = []\n",
    "for i, entry in enumerate(keywords):\n",
    "    if entry is None:\n",
    "        bad_entries.append(i)\n",
    "print(len(bad_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "bad_entries = []\n",
    "for i, entry in enumerate(keywords):\n",
    "    for key in entry:\n",
    "        for keyword in entry[key]:\n",
    "            if not isinstance(keyword, str) or len(keyword) < 2:\n",
    "                bad_entries.append(i)\n",
    "                stop = True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "print(bad_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "bad_entries = []\n",
    "for i, entry in enumerate(keywords):\n",
    "    for key in entry:\n",
    "        if type(entry[key]) != list:\n",
    "                bad_entries.append(i)\n",
    "                break\n",
    "print(bad_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bad_index in bad_entries:\n",
    "    extractive_dict = keywords[bad_index][\"extractive\"]\n",
    "    values = list(extractive_dict.values())\n",
    "    # flatten the list\n",
    "    values = [item for sublist in values for item in sublist]\n",
    "    keywords[bad_index][\"extractive\"] = values\n",
    "\n",
    "    abstractive_dict = keywords[bad_index][\"abstractive\"]\n",
    "    values = list(abstractive_dict.values())\n",
    "    # flatten the list\n",
    "    values = [item for sublist in values for item in sublist]\n",
    "    keywords[bad_index][\"abstractive\"] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractive_kw_lengths = []\n",
    "abstractive_kw_lengths = []\n",
    "\n",
    "for entry in keywords:\n",
    "    extractive_kw_lengths.append(len(entry[\"extractive\"]))\n",
    "    abstractive_kw_lengths.append(len(entry[\"abstractive\"]))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# visualize distribution of lengths using boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the extractive keyword lengths\n",
    "sns.boxplot(extractive_kw_lengths, ax=ax[0])\n",
    "ax[0].set_title('Distribution of Extractive Keyword Lengths')\n",
    "ax[0].set_xlabel('Length')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the abstractive keyword lengths\n",
    "sns.boxplot(abstractive_kw_lengths, ax=ax[1])\n",
    "ax[1].set_title('Distribution of Abstractive Keyword Lengths')\n",
    "ax[1].set_xlabel('Length')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "\n",
    "# Set the main title for the figure\n",
    "fig.suptitle('Keyword Length Distributions')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.pkl\", \"rb\") as f:\n",
    "    keywords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = []\n",
    "for index, entry in enumerate(keywords):\n",
    "    values = list(entry.values())\n",
    "    values_flat = values[0] + values[1]\n",
    "    for value in values_flat:\n",
    "        keywords_flat.append({\"keyword\": value, \"product_index\": index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5488\n",
      "5339\n"
     ]
    }
   ],
   "source": [
    "print(len({entry[\"keyword\"] for entry in keywords_flat}))\n",
    "keywords_reduced, unique_keywords = reduce_keys(keywords_flat, \"keyword\", similarity_threshold=90)\n",
    "print(len(unique_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59be01189f54aa6bc61947710c7c9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2618605c1ac64fd4b3dab881c05a2299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete existing Keyword nodes\n",
    "delete_query = \"\"\"\n",
    "MATCH (k:Keyword)\n",
    "CALL (k) {\n",
    "DETACH DELETE k\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "graphdb.query(delete_query)\n",
    "\n",
    "# Create Keyword nodes\n",
    "keyword_query = \"\"\"\n",
    "UNWIND $keywords AS keyword\n",
    "CREATE (k:Keyword {name: keyword})\n",
    "\"\"\"\n",
    "batch_size = 1000\n",
    "unique_keywords = list(unique_keywords)\n",
    "for i in trange(0, len(unique_keywords), batch_size):\n",
    "    batch = unique_keywords[i:i + batch_size]\n",
    "    graphdb.query(keyword_query, keywords=batch)\n",
    "\n",
    "# Create HAS_KEYWORD relationship between Product (product_df[i]) and Keyword (new_keywords[i])\n",
    "product_keyword_rels = [{\"product_id\": products_df.loc[entry[\"product_index\"], \"parent_asin\"], \"keyword\": entry[\"keyword\"]} for entry in keywords_reduced]\n",
    "\n",
    "product_keyword_query = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (p:Product {product_id: rel.product_id})\n",
    "MATCH (k:Keyword {name: rel.keyword})\n",
    "CREATE (p)-[:HAS_KEYWORD]->(k)\n",
    "\"\"\"\n",
    "for i in trange(0, len(product_keyword_rels), batch_size):\n",
    "    batch = product_keyword_rels[i:i + batch_size]\n",
    "    graphdb.query(product_keyword_query, relationships=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Attribute Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With generative AI, we may significantly enhance the quality of catalog data. Here is an example of using LLM to tag products with specific features. In a typical catalog, a product is listed with a name and description. For example:\n",
    "\n",
    "**Product name**:\n",
    "*Swiffer Antibacterial Cleaner, Febreze Citrus & Light Scent, Refill*\n",
    "\n",
    "**Product description**:\n",
    "*Kills 99.9% of bacteria (Kills 99.9% of staphylococcus aureus and enterobacter aerogenes). Helps eliminate odors in the air with a fresh scent. Great for vinyl, glazed ceramic, sealed marble, laminate, and finished wood floors. Do not use on unfinished, oiled, or waxed wooden boards, non-sealed tiles, or carpeted floors because they may be water sensitive. Good Housekeeping: Since 1909. Limited warranty to consumers. Replacement or refund if defective. Contains no phosphate, chlorine bleach or ammonia. Questions? 1–800–742–9220. www.swiffer.com. Bottle fits all Swiffer Wet Jet devices”*\n",
    "\n",
    "By utilizing an LLM, we can extract and tag key attributes from this information, improving the product’s visibility in search results and its alignment with user needs. The LLM might generate output like this:\n",
    "\n",
    "    {\n",
    "    \"Antibacterial\": true,\n",
    "    \"Scent\": \"Febreze Citrus & Light\",\n",
    "    \"Suitable Floor Types\": [\"Vinyl\", \"Glazed Ceramic\", \"Sealed Marble\", \"Laminate\", \"Finished Wood\"],\n",
    "    \"Not Suitable Floor Types\": [\"Unfinished, Oiled, or Waxed Wooden Boards\", \"Non-Sealed Tiles\", \"Carpeted Floors\"],\n",
    "    \"Phosphate Free\": true,\n",
    "    \"Chlorine Bleach Free\": true,\n",
    "    \"Ammonia Free\": true,\n",
    "    \"Brand\": \"Swiffer\",\n",
    "    \"Compatibility\": [\"Fits all Swiffer Wet Jet devices\"],\n",
    "    \"Product Type\": \"Floor Cleaner\",\n",
    "    \"Package Type\": \"Refill\"\n",
    "    }\n",
    "\n",
    "This detailed and structured output not only aids in better product discovery but also ensures a more precise match with customer queries and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "prompt_text = \"\"\"You are an advanced language model tasked with enhancing product catalogs by extracting key attributes from product titles and descriptions. You will be provided with an existing dictionary of attributes, which may sometimes be empty. Your goal is to analyze the product title and description to identify and add any missing attributes, or modify existing, ensuring the dictionary is accurate and comprehensive.\n",
    "\n",
    "Here is an example of how to process a product's information:\n",
    "\n",
    "**Product Title**: Swiffer Antibacterial Cleaner, Febreze Citrus & Light Scent, Refill  \n",
    "**Product Description**: Kills 99.9% of bacteria (Kills 99.9% of staphylococcus aureus and enterobacter aerogenes). Helps eliminate odors in the air with a fresh scent. Great for vinyl, glazed ceramic, sealed marble, laminate, and finished wood floors. Do not use on unfinished, oiled, or waxed wooden boards, non-sealed tiles, or carpeted floors because they may be water sensitive. Good Housekeeping: Since 1909. Limited warranty to consumers. Replacement or refund if defective. Contains no phosphate, chlorine bleach or ammonia. Questions? 1–800–742–9220. www.swiffer.com. Bottle fits all Swiffer Wet Jet devices.\n",
    "\n",
    "**Key Attributes**:\n",
    "```json\n",
    "{{\n",
    "    \"Antibacterial\": true,\n",
    "    \"Scent\": \"Febreze Citrus & Light\",\n",
    "    \"Suitable Floor Types\": [\"Vinyl\", \"Glazed Ceramic\", \"Sealed Marble\", \"Laminate\", \"Finished Wood\"],\n",
    "    \"Not Suitable Floor Types\": [\"Unfinished, Oiled, or Waxed Wooden Boards\", \"Non-Sealed Tiles\", \"Carpeted Floors\"],\n",
    "    \"Phosphate Free\": true,\n",
    "    \"Chlorine Bleach Free\": true,\n",
    "    \"Ammonia Free\": true,\n",
    "    \"Brand\": \"Swiffer\",\n",
    "    \"Compatibility\": \"Fits all Swiffer Wet Jet devices\",\n",
    "    \"Product Type\": \"Floor Cleaner\",\n",
    "    \"Package Type\": \"Refill\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Using this as a guide, analyze the product title and description provided below to extract relevant attributes and populate the existing dictionary accordingly. Add new attributes if necessary, but ensure that:\n",
    "\n",
    "- All values are either **primitive types** (strings, booleans, numbers) or **lists**.\n",
    "- Do not produce nested structures (like dictionaries or complex objects).\n",
    "- If you find a delimiter (e.g., comma) in the text or a value in the existing attributes, reformat it as a list instead.\n",
    "- Output the dictionary in JSON format with only primitive or list values. Do not include length 1 lists for single values. Use the primitives instead.\n",
    "- Keep the keys in the dictionary consistent with the existing attributes or simplify them. If you find a new attribute, add it to the dictionary with the correct key name.\n",
    "\n",
    "For example, this Key Attributes result is wrong\n",
    "```json\n",
    "{{\n",
    "    \"Age Range (Description)\": \"Adult\", # key should be simplified to \"Age Range\"\n",
    "    \"Product\": {{\"Brand\": \"COVERGIRL\", \"Name\": \"Tone Rehab 2-in-1 Foundation\"}} # dictionaries are not allowed\n",
    "    \"Item Form\": \"Liquid\",\n",
    "    \"Package Dimensions\": [\"5.9\", \"2.3\", \"2 inches\"],\n",
    "    \"Scent\": \"Banana\",\n",
    "    \"Special Feature\": [\"Scented\"], # value should be \"Scented\"\n",
    "    \"UPC\": \"678634485830\",\n",
    "    \"Weight\": [\"4.8 Ounces\"], # value should be \"4.8 Ounces\"\n",
    "}}\n",
    "```\n",
    "\n",
    "This is correct\n",
    "```json\n",
    "{{\n",
    "    \"Age Range\": \"Adult\",\n",
    "    \"Brand\": \"COVERGIRL\",\n",
    "    \"Name\": \"Tone Rehab 2-in-1 Foundation\",\n",
    "    \"Item Form\": \"Liquid\",\n",
    "    \"Package Dimensions\": [\"5.9\", \"2.3\", \"2 inches\"],\n",
    "    \"Scent\": \"Banana\",\n",
    "    \"Special Feature\": \"Scented\",\n",
    "    \"UPC\": \"678634485830\",\n",
    "    \"Weight\": \"4.8 Ounces\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Product Title**: {product_title}  \n",
    "**Product Description**: {product_description}  \n",
    "**Existing Attributes**: {product_details}\n",
    "\n",
    "Generate only the updated dictionary inside a json block as shown in the examples above.\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt_text,\n",
    "    input_variables=[\"product_title\", \"product_description\", \"product_details\"],\n",
    "    input_types={\n",
    "        \"product_title\": str,\n",
    "        \"product_description\": str,\n",
    "        \"product_details\": Dict[str, Any],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral-nemo\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "# capture everything between ```json and ```)\n",
    "json_pattern = re.compile(r'(?<=```json)(.*?)(?=```)', re.DOTALL)\n",
    "\n",
    "def parser(message: AIMessage) -> Dict[str, Any]:\n",
    "    attributes = {}\n",
    "    # find first match\n",
    "    match = json_pattern.search(message.content)\n",
    "    if match:\n",
    "        attributes_str = match.group(0).strip()\n",
    "        try:\n",
    "            attributes = literal_eval(attributes_str)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                attributes = json.loads(attributes_str)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "                # raise ValueError(f\"Failed to parse the attributes dictionary {attributes_str}\")\n",
    "    return attributes\n",
    "\n",
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "\n",
    "def run_example(example=None):\n",
    "    set_debug(True)\n",
    "    if example is not None: \n",
    "        example = products_df.sample()\n",
    "    features = example[\"details\"].values[0]\n",
    "    features[\"store\"] = example[\"store\"].values[0]\n",
    "    product_tile = example[\"title\"].values[0]\n",
    "    product_description = \" \".join(example[\"description\"].values[0])\n",
    "\n",
    "    inputs = {\n",
    "        \"product_title\": product_tile,\n",
    "        \"product_description\": product_description,\n",
    "        \"product_details\": features,\n",
    "    }\n",
    "    result = chain.invoke(inputs)\n",
    "    set_debug(False)\n",
    "    return result\n",
    "\n",
    "# result = run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_attributes = []\n",
    "errors = []\n",
    "\n",
    "output_filename = \"product_attributes.json\"\n",
    "\n",
    "for index, row in tqdm(products_df.iterrows(), total=products_df.shape[0]):\n",
    "    features = literal_eval(row[\"details\"])\n",
    "    features[\"store\"] = row[\"store\"]\n",
    "    product_tile = row[\"title\"]\n",
    "    product_description = \" \".join(row[\"description\"])\n",
    "    inputs = {\n",
    "        \"product_title\": product_tile,\n",
    "        \"product_description\": product_description,\n",
    "        \"product_details\": features,\n",
    "    }\n",
    "    try:\n",
    "        result = chain.invoke(inputs)\n",
    "        product_attributes.append(result)\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            json.dump(product_attributes, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {index}: {e}\")\n",
    "        errors.append(index)\n",
    "        product_attributes.append(None)\n",
    "\n",
    "with open(f\"errors_{output_filename[:-4]}.txt\", \"w\") as f:\n",
    "    for error in errors:\n",
    "        f.write(f\"{error}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resolve errors here\n",
    "with open(f\"errors_{output_filename[:-4]}.txt\") as f:\n",
    "    errors = f.read().splitlines()\n",
    "    errors = [int(e) for e in errors]\n",
    "\n",
    "with open(output_filename) as f:\n",
    "    product_attributes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_output(output):\n",
    "    for key in output:\n",
    "        if type(output[key]) not in (str, bool, int, list):\n",
    "            return False\n",
    "        if type(output[key]) == list:\n",
    "            for element in output[key]:\n",
    "                if type(element) not in (str, bool, int):\n",
    "                    return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in tqdm(errors):\n",
    "    if index in solved:\n",
    "        continue\n",
    "    \n",
    "    row = products_df.loc[index]\n",
    "    features = row[\"details\"]\n",
    "    features[\"store\"] = row[\"store\"]\n",
    "    product_tile = row[\"title\"]\n",
    "    product_description = \" \".join(row[\"description\"])\n",
    "    inputs = {\n",
    "        \"product_title\": product_tile,\n",
    "        \"product_description\": product_description,\n",
    "        \"product_details\": features,\n",
    "    }\n",
    "    i = 0\n",
    "    while i < 5:\n",
    "        try:\n",
    "            result = chain.invoke(inputs)\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error for product {index} on attempt {i + 1}\")\n",
    "            i += 1\n",
    "            continue\n",
    "        if result and validate_output(result):\n",
    "            product_attributes[index] = result\n",
    "            solved.append(index)\n",
    "            print(f\"Fixed error for product {index} on attempt {i + 1}\")\n",
    "            break\n",
    "        i += 1\n",
    "        print(f\"Attempt {i} failed for product {index}\")\n",
    "if len(solved) != len(errors):\n",
    "    print(f\"Fixed {len(solved)} bad entries! {len(errors) - len(solved)} remaining...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"product_attributes.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(product_attributes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, entry in enumerate(product_attributes):\n",
    "    if type(entry) == list:\n",
    "        for j, element in enumerate(entry):\n",
    "            try:\n",
    "                # try to convert to float if it's not a boolean\n",
    "                if type(element) != bool:\n",
    "                    element = float(element)\n",
    "            except ValueError:\n",
    "                # in case of string\n",
    "                pass\n",
    "            else:\n",
    "                # if successful, replace the element with the float\n",
    "                entry[j] = element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"product_attributes.json\", \"r\") as f:\n",
    "    product_attributes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_set = set()\n",
    "attributes_data = []\n",
    "relationships = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in products_df.iterrows():\n",
    "    product_id = row[\"parent_asin\"]\n",
    "    product_details = product_attributes[index]\n",
    "\n",
    "    for key, value in product_details.items():\n",
    "        # Escape single quotes in the key\n",
    "        key = key.replace(\"'\", \"\\\\'\")\n",
    "\n",
    "        if type(value) == list:\n",
    "            for element in value:\n",
    "                # Add attribute node data \n",
    "                attribute_tuple = (key, element)\n",
    "                attribute = {\n",
    "                    'name': key,\n",
    "                    'value': element\n",
    "                }\n",
    "                if attribute_tuple not in attributes_set:\n",
    "                    attributes_data.append(attribute)\n",
    "                    attributes_set.add(attribute_tuple)\n",
    "\n",
    "                # Collect relationships between product and attribute nodes\n",
    "                relationships.append({\n",
    "                    'product_id': product_id,\n",
    "                    'attribute_name': key,\n",
    "                    'attribute_value': element\n",
    "                })\n",
    "        else:\n",
    "            # Add attribute node data \n",
    "            attribute_tuple = (key, value)\n",
    "            attribute = {\n",
    "                'name': key,\n",
    "                'value': value\n",
    "            }\n",
    "            \n",
    "            if attribute_tuple not in attributes_set:\n",
    "                attributes_data.append(attribute)\n",
    "                attributes_set.add(attribute_tuple)\n",
    "\n",
    "            # Collect relationships between product and attribute nodes\n",
    "            relationships.append({\n",
    "                'product_id': product_id,\n",
    "                'attribute_name': key,\n",
    "                'attribute_value': value\n",
    "            })\n",
    "\n",
    "# Delete existing Attribute nodes\n",
    "delete_query = \"\"\"\n",
    "MATCH (a:Attribute)\n",
    "CALL (a) { \n",
    "DETACH DELETE a\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "graphdb.query(\"MATCH (a:Attribute) DETACH DELETE a\")\n",
    "# Create Attribute nodes\n",
    "attribute_query = \"\"\"\n",
    "UNWIND $attributes AS attr\n",
    "CREATE (a:Attribute {name: attr.name, value: attr.value})\n",
    "\"\"\"\n",
    "# Insert attribute nodes\n",
    "for i in trange(0, len(attributes_data), batch_size):\n",
    "    batch = attributes_data[i:i + batch_size]\n",
    "    graphdb.query(attribute_query.strip(), attributes=batch)\n",
    "\n",
    "# Create indexes for the Attribute nodes\n",
    "graphdb.query(\"DROP INDEX attribute_range_name IF EXISTS\")\n",
    "graphdb.query(\"CREATE INDEX attribute_range_name FOR (a:Attribute) ON (a.name)\")\n",
    "# Create HAS_ATTRIBUTE relationships\n",
    "relationship_query = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (p:Product {product_id: rel.product_id})\n",
    "MATCH (a:Attribute {name: rel.attribute_name, value: rel.attribute_value})\n",
    "MERGE (p)-[:HAS_ATTRIBUTE]->(a)\n",
    "\"\"\"\n",
    "# Create relationships between Product and Attribute nodes\n",
    "for i in trange(0, len(relationships), batch_size):\n",
    "    batch = relationships[i:i + batch_size]\n",
    "    graphdb.query(relationship_query.strip(), relationships=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create **summary** of product from all of its attributes using a template. Then, create **embedding** of this summary for vector search. Use it to find products that are **substitutes** or **complements** of a product. A product is a substitute if it belongs to the same (or similar) categories, otherwise it is a complement. Complement can also be determined (better) via `bought_together`, however this is usually NULL. Another (hacky) technique for complements is to use an LLM to suggest titles for complements (\"complements\", \"frequently used with\", or \"often mentioned together\"), adding these edges to the KG, and when searching for complements, use full text search to find products that match these titles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of a product embedding requires us to take more information into consideration. Most current systems use only product names to produce the embedding, which fails to fully capture the product’s characteristics. It’s more effective to compile all the available data about a product and convert this complete information into text. This can be achieved by joining all related tables — including product, brand, category, taxonomy, and others — into a singular, “flattened” table.\n",
    "\n",
    "The next step involves transforming the structured data in the “flattened” table into a textual format. The text is then converted into embeddings and stored in the vector database. Essentially, the LLM, combining the semantics represented by the table schema with the specific data values of each product, crafts a natural language description of the product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_description(row):\n",
    "    \"\"\"\n",
    "    Generate a product description using a template from a DataFrame row.\n",
    "\n",
    "    Parameters:\n",
    "    row (pd.Series): A row from the DataFrame containing product data.\n",
    "\n",
    "    Returns:\n",
    "    str: A product description generated according to Template 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract mandatory fields with default fallbacks\n",
    "    product_title = row.get(\"title\", \"N/A\")\n",
    "    main_category = row.get(\"main_category\", \"N/A\")\n",
    "    brand_store = row.get(\"store\", \"N/A\")\n",
    "    price = row.get(\"price\", \"N/A\")\n",
    "    average_rating = row.get(\"average_rating\", \"N/A\")\n",
    "    rating_number = row.get(\"rating_number\", \"N/A\")\n",
    "\n",
    "    # Parse 'categories' field\n",
    "    categories = row.get(\"categories\", [])\n",
    "    if isinstance(categories, str):\n",
    "        try:\n",
    "            categories = literal_eval(categories)\n",
    "        except (ValueError, SyntaxError):\n",
    "            categories = [categories]\n",
    "    categories_str = \", \".join(categories) if categories else \"N/A\"\n",
    "\n",
    "    # Parse 'details' field\n",
    "    details = row.get(\"details\", {})\n",
    "    if isinstance(details, str):\n",
    "        try:\n",
    "            # Clean up the string to make it valid JSON\n",
    "            details = details.replace('\"\"', '\"').replace(\"'\", '\"')\n",
    "            details = json.loads(details)\n",
    "        except json.JSONDecodeError:\n",
    "            details = {}\n",
    "    # Extract optional fields\n",
    "    material = details.get(\"Material\", None)\n",
    "    color = details.get(\"Color\", None)\n",
    "    product_dimensions = details.get(\"Package Dimensions\") or details.get(\n",
    "        \"Product Dimensions\"\n",
    "    )\n",
    "    weight = details.get(\"Item Weight\")\n",
    "\n",
    "    # Prepare 'product features' string\n",
    "    features = row.get(\"features\", [])\n",
    "    if isinstance(features, str):\n",
    "        try:\n",
    "            features = literal_eval(features)\n",
    "        except (ValueError, SyntaxError):\n",
    "            features = [features]\n",
    "    features = [f.strip(\"'\\\"\") for f in features if f]\n",
    "    key_features_str = \"; \".join(features) if features else \"N/A\"\n",
    "\n",
    "    # Collect additional details (excluding already used keys)\n",
    "    used_keys = {\n",
    "        \"Material\",\n",
    "        \"Color\",\n",
    "        \"Package Dimensions\",\n",
    "        \"Product Dimensions\",\n",
    "        \"Item Weight\",\n",
    "        \"Brand\",\n",
    "    }\n",
    "    additional_details = [\n",
    "        f\"{key} - {value}\" for key, value in details.items() if key not in used_keys\n",
    "    ]\n",
    "    additional_details_str = (\n",
    "        \", \".join(additional_details) if additional_details else \"N/A\"\n",
    "    )\n",
    "\n",
    "    # Construct the description using Template 1\n",
    "    description_parts = [\n",
    "        f\"{product_title} is a product under the {main_category} category.\",\n",
    "        f\"Manufactured by {brand_store}, it is priced at ${price}.\",\n",
    "    ]\n",
    "\n",
    "    # Include optional fields if available\n",
    "    if material or color:\n",
    "        material_color = \" and \".join(filter(None, [material, color]))\n",
    "        description_parts.append(f\"The product features {material_color}.\")\n",
    "    else:\n",
    "        description_parts.append(\"\")\n",
    "\n",
    "    if product_dimensions or weight:\n",
    "        dimensions_weight = \" and \".join(\n",
    "            filter(\n",
    "                None,\n",
    "                [\n",
    "                    (\n",
    "                        f\"dimensions of {product_dimensions}\"\n",
    "                        if product_dimensions\n",
    "                        else None\n",
    "                    ),\n",
    "                    f\"weighs {weight}\" if weight else None,\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        description_parts.append(f\"It has {dimensions_weight}.\")\n",
    "    else:\n",
    "        description_parts.append(\"\")\n",
    "\n",
    "    description_parts.append(f\"Key features include {key_features_str}.\")\n",
    "    description_parts.append(\n",
    "        f\"It has an average rating of {average_rating} stars based on {rating_number} reviews.\"\n",
    "    )\n",
    "    description_parts.append(f\"It falls under the subcategories: {categories_str}.\")\n",
    "    description_parts.append(f\"Additional details: {additional_details_str}.\")\n",
    "\n",
    "    # Filter out any empty strings\n",
    "    description_parts = [part for part in description_parts if part]\n",
    "\n",
    "    # Join all parts into the final description\n",
    "    description = \" \".join(description_parts)\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = products_df.sample(1).iloc[0]\n",
    "description = generate_product_description(row)\n",
    "print(len(description))\n",
    "pprint(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ProductSummary(BaseModel):\n",
    "    summary: str = Field(\n",
    "        title=\"summary\",\n",
    "        description=\"A concise, holistic, and natural sounding summary of the product.\",\n",
    "        min_length=10\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_text = \"\"\"Rephrase the following product description to create a natural, human-friendly narrative while ensuring it provides detailed information for product research. \n",
    "The description should flow naturally, avoiding overly technical language or mechanical listing of features, but must still be comprehensive enough to allow for detailed product analysis.\n",
    "\n",
    "Ensure the description includes the following:\n",
    "- Product design, material, and functionality.\n",
    "- Key features and how they benefit the user.\n",
    "- Primary use cases and scenarios where the product excels.\n",
    "- Compatibility with related products or items in the same category.\n",
    "\n",
    "Guidelines:\n",
    "- Keep the tone neutral and informative, focusing on providing clear, useful information that would help in comparing this product with others.\n",
    "- Avoid using a salesy or promotional tone. \n",
    "- The description should feel natural and readable, but the completeness of information is the priority.\n",
    "- Do not generate the result using markdown\n",
    "\n",
    "Product Description: ```{description}```\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt_text,\n",
    "    input_variables=[\"description\"],\n",
    "    input_types={\"description\": str},\n",
    ")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral-nemo\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "summary_llm = llm.with_structured_output(ProductSummary, include_raw=True)\n",
    "summary_chain = prompt_template | summary_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "def run_example(example=None):\n",
    "    set_debug(True)\n",
    "\n",
    "    if example is None:\n",
    "        example = products_df.sample().iloc[0]\n",
    "        print(example.name)\n",
    "    \n",
    "    product_description = generate_product_description(example)\n",
    "    result = summary_chain.invoke({\"description\": product_description})\n",
    "    \n",
    "    set_debug(False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['raw'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(input_str):\n",
    "    \"\"\"\n",
    "    Preprocesses the input string by extracting and escaping the \"summary\" field.\n",
    "\n",
    "    Steps:\n",
    "    1. Check if the string starts with [TOOL_CALLS]. If not, return None.\n",
    "    2. Remove the [TOOL_CALLS] prefix.\n",
    "    3. Manually extract the \"summary\" value, accounting for unescaped quotes.\n",
    "    4. Escape backslashes, double quotes, and newline characters within the summary.\n",
    "\n",
    "    Args:\n",
    "        input_str (str): The input string to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The escaped summary string if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    PREFIX = '[TOOL_CALLS]'\n",
    "\n",
    "    # Step 1: Check for the prefix\n",
    "    if not input_str.startswith(PREFIX):\n",
    "        return None\n",
    "\n",
    "    # Step 2: Remove the prefix\n",
    "    json_str = input_str[len(PREFIX):]\n",
    "\n",
    "    # Step 3: Manually extract the summary value\n",
    "    summary_key = '\"summary\": \"'\n",
    "    start_idx = json_str.find(summary_key)\n",
    "    if start_idx == -1:\n",
    "        return None\n",
    "    start_idx += len(summary_key)\n",
    "\n",
    "    # Find the index of the closing quote before '}}]'\n",
    "    end_marker = '\"}}]'\n",
    "    end_idx = json_str.rfind(end_marker)\n",
    "    if end_idx == -1:\n",
    "        return None\n",
    "\n",
    "    # Extract the raw summary text\n",
    "    summary_text = json_str[start_idx:end_idx]\n",
    "\n",
    "    # Step 4: Escape necessary characters\n",
    "    summary_text = summary_text.replace('\\\\', '\\\\\\\\')  # Escape backslashes first\n",
    "    summary_text = summary_text.replace('\"', '\\\\\"')    # Escape double quotes\n",
    "    summary_text = summary_text.replace('\\n', '\\\\n')   # Escape newlines\n",
    "\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = preprocess_string(result['raw'].content)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/summaries.json\") as f:\n",
    "    summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### error resolution...\n",
    "with open(\"data/errors_summaries.txt\") as f:\n",
    "    errors = f.read().splitlines()\n",
    "    errors = [int(e) for e in errors]\n",
    "print(errors)\n",
    "\n",
    "solved = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = errors[2] # 0, 1, 3, 4\n",
    "row = products_df.loc[index]\n",
    "result = run_example(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['parsed'])\n",
    "\n",
    "if result['parsed']:\n",
    "    summaries.append({\"index\": index, \"summary\": result['parsed'].summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_result = result['raw'].content\n",
    "print(raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_result = raw_result[raw_result.find(\"Product Summary:\") + len(\"Product Summary:\"):].strip()\n",
    "print(raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_result_new = raw_result.replace(\"\\n\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_result_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_result = preprocess_string(raw_result)\n",
    "print(preprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries.append({\"index\": index, \"summary\": \"The ARTERO X-Tron Professional Hairstyling Clippers are designed for flexibility, with both cordless and plug-in use. They offer three adjustable speeds (5500, 6000, 6500 RPM) displayed on an LCD screen and come with comb attachments for trimming heights from 3mm to 12mm. A lithium battery provides up to 150 minutes of runtime, with a quick 90-minute charge. The clippers are lightweight and suitable for all hair types, making them ideal for professional use. Maintenance is simple with the included mineral oil and compatibility with ARTERO's Oil Fresh Spray for cleaning and disinfecting.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/summaries.json\", \"w\") as f:\n",
    "    json.dump(summaries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Fresh Beards Classic Beard Butter is a met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The DOORES Wire Hair Extensions offer a seamle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fingernail Friends Colorful Nail Stickers is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The MISUD Long Press on Nails is a high-qualit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Shikai Henna Gold Highlighting Shampoo is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>156</td>\n",
       "      <td>Gold Bond's Overnight Deep Moisturizing Lotion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>161</td>\n",
       "      <td>The NOACIER Blemish Defense Night Face Serum i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>156</td>\n",
       "      <td>The Luinabio Chinese Classical Glaze Hair Stic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>681</td>\n",
       "      <td>The Brocato Supersilk Pure Indulgence Shampoo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>263</td>\n",
       "      <td>The ARTERO X-Tron Professional Hairstyling Cli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                            summary\n",
       "0        0  The Fresh Beards Classic Beard Butter is a met...\n",
       "1        1  The DOORES Wire Hair Extensions offer a seamle...\n",
       "2        2  Fingernail Friends Colorful Nail Stickers is a...\n",
       "3        3  The MISUD Long Press on Nails is a high-qualit...\n",
       "4        4  The Shikai Henna Gold Highlighting Shampoo is ...\n",
       "..     ...                                                ...\n",
       "953    156  Gold Bond's Overnight Deep Moisturizing Lotion...\n",
       "954    161  The NOACIER Blemish Defense Night Face Serum i...\n",
       "955    156  The Luinabio Chinese Classical Glaze Hair Stic...\n",
       "956    681  The Brocato Supersilk Pure Indulgence Shampoo ...\n",
       "957    263  The ARTERO X-Tron Professional Hairstyling Cli...\n",
       "\n",
       "[958 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = pd.read_json(\"data/summaries.json\")\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All Beauty</td>\n",
       "      <td>Fresh Beards Classic Beard Butter - Unscented ...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>600</td>\n",
       "      <td>19.99</td>\n",
       "      <td>{'hi_res': array(['https://m.media-amazon.com/...</td>\n",
       "      <td>Fresh Beards</td>\n",
       "      <td>[Shave &amp; Hair Removal, Men's, Beard &amp; Mustache...</td>\n",
       "      <td>{'Brand': 'Fresh Beards', 'Item Form': 'Butter...</td>\n",
       "      <td>B0BCXCYS2Q</td>\n",
       "      <td>[CLASSIC BEARD BUTTER: Our original unscented ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Fresh Beards Classic Beard Butter is a met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All Beauty</td>\n",
       "      <td>DOORES Hair Extensions Wire Hair Extensions Ba...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2019</td>\n",
       "      <td>98.99</td>\n",
       "      <td>{'hi_res': array(['https://m.media-amazon.com/...</td>\n",
       "      <td>DOORES</td>\n",
       "      <td>[Hair Care, Hair Extensions, Wigs &amp; Accessorie...</td>\n",
       "      <td>{'Brand': 'DOORES', 'Color': '#(2/6)/2 Dark Br...</td>\n",
       "      <td>B08LKG6CWL</td>\n",
       "      <td>[1.【100% Human Hair Extensions - 9A Salon Qual...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The DOORES Wire Hair Extensions offer a seamle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Beauty</td>\n",
       "      <td>Fingernail Friends Colorful Nail Stickers Nail...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>11.99</td>\n",
       "      <td>{'hi_res': array(['https://m.media-amazon.com/...</td>\n",
       "      <td>allydrew</td>\n",
       "      <td>[Foot, Hand &amp; Nail Care, Nail Art &amp; Polish, Na...</td>\n",
       "      <td>{'Brand': 'Allydrew', 'Color': '2 Pack', 'Item...</td>\n",
       "      <td>B07PNRZLBH</td>\n",
       "      <td>[Includes: 50 Fairytale Nail Stickers &amp; 50 Cut...</td>\n",
       "      <td>[Our Fingernail Friends Colorful Nail Stickers...</td>\n",
       "      <td>Fingernail Friends Colorful Nail Stickers is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All Beauty</td>\n",
       "      <td>MISUD Long Press on Nails, Coffin Fake Nails, ...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>372</td>\n",
       "      <td>7.66</td>\n",
       "      <td>{'hi_res': array(['https://m.media-amazon.com/...</td>\n",
       "      <td>MISUD</td>\n",
       "      <td>[Foot, Hand &amp; Nail Care, Nail Art &amp; Polish, Fa...</td>\n",
       "      <td>{'Color': 'Blue', 'Size': 'Long, Coffin/Baller...</td>\n",
       "      <td>B08ZSB814R</td>\n",
       "      <td>[✨[HIGH QUALITY] -Our nails are made with high...</td>\n",
       "      <td>[PACKAGE CONTENTS:1 X 24pcs Fake Nails 1 X Dou...</td>\n",
       "      <td>The MISUD Long Press on Nails is a high-qualit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All Beauty</td>\n",
       "      <td>Shikai - Henna Gold Highlighting Shampoo, Brin...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>110</td>\n",
       "      <td>36.00</td>\n",
       "      <td>{'hi_res': array(['https://m.media-amazon.com/...</td>\n",
       "      <td>ShiKai</td>\n",
       "      <td>[Hair Care, Hair Coloring Products, Hennas]</td>\n",
       "      <td>{'Is Discontinued By Manufacturer': 'No', 'Pro...</td>\n",
       "      <td>B018RLF49U</td>\n",
       "      <td>[TURN ON THE LIGHTS TO YOUR HAIR’S SHIMMERING ...</td>\n",
       "      <td>[In 1970, Dr. Dennis Sepp, an organic chemist,...</td>\n",
       "      <td>The Shikai Henna Gold Highlighting Shampoo is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category                                              title  \\\n",
       "0    All Beauty  Fresh Beards Classic Beard Butter - Unscented ...   \n",
       "1    All Beauty  DOORES Hair Extensions Wire Hair Extensions Ba...   \n",
       "2    All Beauty  Fingernail Friends Colorful Nail Stickers Nail...   \n",
       "3    All Beauty  MISUD Long Press on Nails, Coffin Fake Nails, ...   \n",
       "4    All Beauty  Shikai - Henna Gold Highlighting Shampoo, Brin...   \n",
       "\n",
       "   average_rating  rating_number  price  \\\n",
       "0             4.6            600  19.99   \n",
       "1             4.3           2019  98.99   \n",
       "2             3.0              9  11.99   \n",
       "3             3.8            372   7.66   \n",
       "4             4.4            110  36.00   \n",
       "\n",
       "                                              images         store  \\\n",
       "0  {'hi_res': array(['https://m.media-amazon.com/...  Fresh Beards   \n",
       "1  {'hi_res': array(['https://m.media-amazon.com/...        DOORES   \n",
       "2  {'hi_res': array(['https://m.media-amazon.com/...      allydrew   \n",
       "3  {'hi_res': array(['https://m.media-amazon.com/...         MISUD   \n",
       "4  {'hi_res': array(['https://m.media-amazon.com/...        ShiKai   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [Shave & Hair Removal, Men's, Beard & Mustache...   \n",
       "1  [Hair Care, Hair Extensions, Wigs & Accessorie...   \n",
       "2  [Foot, Hand & Nail Care, Nail Art & Polish, Na...   \n",
       "3  [Foot, Hand & Nail Care, Nail Art & Polish, Fa...   \n",
       "4        [Hair Care, Hair Coloring Products, Hennas]   \n",
       "\n",
       "                                             details parent_asin  \\\n",
       "0  {'Brand': 'Fresh Beards', 'Item Form': 'Butter...  B0BCXCYS2Q   \n",
       "1  {'Brand': 'DOORES', 'Color': '#(2/6)/2 Dark Br...  B08LKG6CWL   \n",
       "2  {'Brand': 'Allydrew', 'Color': '2 Pack', 'Item...  B07PNRZLBH   \n",
       "3  {'Color': 'Blue', 'Size': 'Long, Coffin/Baller...  B08ZSB814R   \n",
       "4  {'Is Discontinued By Manufacturer': 'No', 'Pro...  B018RLF49U   \n",
       "\n",
       "                                            features  \\\n",
       "0  [CLASSIC BEARD BUTTER: Our original unscented ...   \n",
       "1  [1.【100% Human Hair Extensions - 9A Salon Qual...   \n",
       "2  [Includes: 50 Fairytale Nail Stickers & 50 Cut...   \n",
       "3  [✨[HIGH QUALITY] -Our nails are made with high...   \n",
       "4  [TURN ON THE LIGHTS TO YOUR HAIR’S SHIMMERING ...   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [Our Fingernail Friends Colorful Nail Stickers...   \n",
       "3  [PACKAGE CONTENTS:1 X 24pcs Fake Nails 1 X Dou...   \n",
       "4  [In 1970, Dr. Dennis Sepp, an organic chemist,...   \n",
       "\n",
       "                                             summary  \n",
       "0  The Fresh Beards Classic Beard Butter is a met...  \n",
       "1  The DOORES Wire Hair Extensions offer a seamle...  \n",
       "2  Fingernail Friends Colorful Nail Stickers is a...  \n",
       "3  The MISUD Long Press on Nails is a high-qualit...  \n",
       "4  The Shikai Henna Gold Highlighting Shampoo is ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_summaries_df = pd.merge(products_df.reset_index(), summaries, on=\"index\").drop(columns=[\"index\"])\n",
    "product_summaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_summary = []\n",
    "\n",
    "for _, row in product_summaries_df.iterrows():\n",
    "    product_id = row[\"parent_asin\"]\n",
    "    summary = row[\"summary\"]\n",
    "\n",
    "    pid_summary.append({\"product_id\": product_id, \"summary\": summary})\n",
    "\n",
    "# Set summary property of Product nodes\n",
    "summary_query = \"\"\"\n",
    "UNWIND $summaries AS summary\n",
    "MATCH (p:Product {product_id: summary.product_id})\n",
    "SET p.summary = summary.summary\n",
    "\"\"\"\n",
    "graphdb.query(summary_query, summaries=pid_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow**:\n",
    "\n",
    "*Use Case Nodes*: Create nodes in the KG that represent common use cases for products. These nodes connect user intents to specific product interactions.\n",
    "\n",
    "*Example*: A query for “insomnia” could pull up a use case node linked to “Epsom salt baths for sleep,” with a description of how and why the product fits the use case. Queries can be mapped to use-cases via semantic search (query -> generated use case -> semantic search on db of use-cases). Other examples of intent: “office chair for back pain,” “eco-friendly cleaning supplies”\n",
    "\n",
    "*Narrative Explanation*: Tie each use case node to a narrative or scenario-based explanation that illustrates how the product solves the problem. For example, “Many users find that taking an Epsom salt bath before bed helps them relax and prepare for sleep.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UseCase(BaseModel):\n",
    "    title: str = Field(\n",
    "        title=\"title\",\n",
    "        description=\"A short title for the use case.\",\n",
    "        examples=[\"Relaxation via aromatherapy\", \"Sleep aid\", \"Skin care\"],\n",
    "        min_length=5,\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        title=\"explanation\",\n",
    "        description=\"A brief explanation of why this subcategory is suitable for this use case.\",\n",
    "        examples=[\n",
    "            \"It is commonly used in diffusers to promote relaxation and reduce stress due to its natural calming properties.\",\n",
    "            \"It is known to promote sleep, making it an effective natural remedy for people with insomnia or sleep disorders when diffused at night.\",\n",
    "            \"It has antimicrobial properties, making it useful as a topical treatment for minor skin irritations and acne.\",\n",
    "        ],\n",
    "        min_length=10,\n",
    "    )\n",
    "\n",
    "\n",
    "class UseCaseList(BaseModel):\n",
    "    use_cases: List[UseCase] = Field(\n",
    "        title=\"use_cases\",\n",
    "        description=\"A list of common use cases and explanations for the product.\",\n",
    "        min_length=2,\n",
    "        max_length=10,\n",
    "    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_cases)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.use_cases[item]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.use_cases)\n",
    "    \n",
    "# capture everything between ```json and ```)\n",
    "json_pattern = re.compile(r'(?<=```json)(.*?)(?=```)', re.DOTALL)\n",
    "\n",
    "def usecase_parser(message: AIMessage) -> List[Dict[str, str]]:\n",
    "    usecases = []\n",
    "    # find first match\n",
    "    match = json_pattern.search(message.content)\n",
    "    if match:\n",
    "        usecases_str = match.group(0).strip()\n",
    "        try:\n",
    "            usecases = literal_eval(usecases_str)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                usecases = json.loads(usecases_str)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    return usecases\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "Generate a list of use-cases for the following subcategory of products, given the subcategory title and the main category that it belongs to on an eCommerce website. \n",
    "Each use-case should include a **title** and a brief **explanation** of why it is suitable for that use-case. \n",
    "\n",
    "**Expected Output Format:**\n",
    "```json\n",
    "[\n",
    "    {{\n",
    "      \"title\": \"Relaxation via aromatherapy,\n",
    "      \"explanation\": \"It is commonly used in diffusers to promote relaxation and reduce stress due to its natural calming properties.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"title\": \"Sleep aid\",\n",
    "      \"explanation\": \"It is known to promote sleep, making it an effective natural remedy for people with insomnia or sleep disorders when diffused at night.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"title\": \"Skincare\",\n",
    "      \"explanation\": \"It has antimicrobial properties, making it useful as a topical treatment for minor skin irritations and acne.\"\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "The goal is to provide a comprehensive list of common scenarios where the subcategory of products is used, along with a succinct explanation of why this use case exists.\n",
    "It is recommended to generate at least 5 use-cases. Do not generate less than 2 or exceed 10 use-cases.\n",
    "\n",
    "Subcategory: {subcategory}\n",
    "Main Category: {main_category}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt_text,\n",
    "    input_variables=[\"subcategory\", \"main_category\"],\n",
    "    input_types={\"subcategory\": str, \"main_category\": str},\n",
    ")\n",
    "# usecase_llm = llm.with_structured_output(UseCaseList)\n",
    "usecase_chain = prompt_template | llm | usecase_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(example=None):\n",
    "    set_debug(True)\n",
    "    if not example:\n",
    "        row = products_df.sample().iloc[0]\n",
    "        categories = literal_eval(row[\"categories\"])\n",
    "        subcategory = random.choice(categories)\n",
    "        example = {\"main_category\": row[\"main_category\"], \"subcategory\": subcategory}\n",
    "    \n",
    "    result = usecase_chain.invoke(input=example)\n",
    "    set_debug(False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategories = set()\n",
    "for index, row in products_df.iterrows():\n",
    "    for subcategory in row[\"categories\"]:\n",
    "        subcategories.add((row[\"main_category\"], subcategory))\n",
    "subcategories = list(subcategories)\n",
    "len(subcategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/use_cases.json\"):\n",
    "    with open(\"data/use_cases.json\") as f:\n",
    "        use_cases = json.load(f)\n",
    "else:\n",
    "    use_cases = []\n",
    "\n",
    "errors = []\n",
    "\n",
    "for main_category, subcategory in tqdm(subcategories):\n",
    "    try:\n",
    "        result = usecase_chain.invoke(\n",
    "            {\"subcategory\": subcategory, \"main_category\": main_category}\n",
    "        )\n",
    "        for usecase in result:\n",
    "            title = usecase[\"title\"]\n",
    "            explanation = usecase[\"explanation\"]\n",
    "            use_cases.append(\n",
    "                {\n",
    "                    \"title\": title,\n",
    "                    \"explanation\": explanation,\n",
    "                    \"subcategory\": subcategory,\n",
    "                    \"main_category\": main_category,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        errors.append(index)\n",
    "        print(f\"Error processing product {index}\")\n",
    "        with open(\"data/errors_use_cases.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(errors))\n",
    "    else:\n",
    "        with open(\"data/use_cases.json\", \"w\") as f:\n",
    "            json.dump(use_cases, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1847\n",
      "1641\n"
     ]
    }
   ],
   "source": [
    "### error resolution...\n",
    "\n",
    "with open(\"data/use_cases.json\", \"r\") as f:\n",
    "    use_cases = json.load(f)\n",
    "\n",
    "print(len({entry['title'] for entry in use_cases}))\n",
    "use_cases, unique_uc = reduce_keys(use_cases, key_field='title', similarity_threshold=90)\n",
    "print(len({entry['title'] for entry in use_cases}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd902e65152d4caeafe2f8b9ee86317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# delete existing use-case nodes\n",
    "delete_query = \"\"\"\n",
    "MATCH (u:UseCase)\n",
    "CALL (u) {\n",
    "DETACH DELETE u\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "graphdb.query(delete_query)\n",
    "\n",
    "create_usecases_query = \"\"\"\n",
    "UNWIND $data AS data\n",
    "// Match the existing Category node\n",
    "MATCH (category:Category {name: data.main_category})\n",
    "// Match the existing Subcategory node\n",
    "MATCH (subcategory:Subcategory {name: data.subcategory})\n",
    "// Create the UseCase node\n",
    "MERGE (usecase:UseCase {title: data.title})\n",
    "// Create the USED_FOR relationship with the explanation property\n",
    "MERGE (usecase)-[:USED_FOR {explanation: data.explanation}]->(subcategory)\n",
    "\"\"\"\n",
    "batch_size = 1000\n",
    "for i in trange(0, len(use_cases), batch_size):\n",
    "    batch = use_cases[i:i+batch_size]\n",
    "    graphdb.query(create_usecases_query, data=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
